{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaisalWani123/DND/blob/main/EXAM_2_PRACTICE_IPYNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p415H870SFoQ"
      },
      "source": [
        "# **Final Exam - Deep Network Development**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhzDq8zcLozF"
      },
      "source": [
        "# **Exam Information**\n",
        "\n",
        "---\n",
        "\n",
        "- **Name:** *<Enter your name here>*\n",
        "- **Neptun ID:** *<Enter your Neptun ID here>*\n",
        "- **Date:** *03/01/2025*\n",
        "- **Duration:** *9:00 AM ‚Äì 11:00 AM*\n",
        "- *Please fill in your details above before starting the exam.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBpfD9z3SiMV"
      },
      "source": [
        "## **General Rules**\n",
        "\n",
        "This notebook contains the task to be completed in order to pass the exam and the course. Below are the details:\n",
        "1. **Implementing a network architecture**, including its **forward pass** function.\n",
        "2. Additional **optional requirements** for bonus points towards final grade.\n",
        "3. You have **2 hours** to complete the exam.\n",
        "4. You may distribute the time as you see fit between the required and optional parts.\n",
        "5. You are allowed to use any resource including: the internet, AI tools, practice notebooks, and more.\n",
        "6. **It is strictly prohibited to use any form of communication** (e.g., Teams, WhatsApp, Messenger, etc.). **Violation will result in an immediate FAIL** of the exam.\n",
        "\n",
        "---\n",
        "\n",
        "### **Submission Guidelines**\n",
        "- Submit your solution as a **`.ipynb` file** on **Canvas**.\n",
        "- To **PASS**, your solution must:\n",
        "  1. **Satisfy the minimum requirements** (i.e., a working implementation of the network architecture and forward pass).\n",
        "  2. Be **submitted on time**.\n",
        "  3. Be prepared to **orally defend your code** after submission.\n",
        "\n",
        "---\n",
        "\n",
        "### **Exam Retake Policy**\n",
        "- If you **FAIL**, you are allowed to do **one retake**.  \n",
        "- If you **FAIL AGAIN**, sadly, you **fail the course**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Grading**\n",
        "- If you **PASS**, your final grade will be the **weighted average** of your assignment defenses (theory and code).\n",
        "\n",
        "---\n",
        "\n",
        "Good luck, and ensure you follow all the rules!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eJ8a7vIUKLh"
      },
      "source": [
        "## **Requirements**\n",
        "\n",
        "---\n",
        "\n",
        "### **Minimum Requirements ‚Äì Sufficient to Pass the Exam**\n",
        "1. **Implement the layers of the architecture:**  \n",
        "   Complete the architecture outlined in Section 1 by filling in the missing parts.\n",
        "2. **Implement the forward function:**  \n",
        "   Ensure the input and output of the forward function are correctly implemented.  \n",
        "   \n",
        "   **Note:** To meet these requirements, your final output must match the expected output.\n",
        "\n",
        "---\n",
        "\n",
        "### **Extra Requirements ‚Äì For Grade Improvement and AI Lab Access**\n",
        "\n",
        "---\n",
        "\n",
        "3. **Text-to-Image with Image-Guided Embeddings:**  \n",
        "- The goal is to perform text-to-image generation using an existing image as a guide for editing. The input text specifies modifications to the existing image, preserving its content while applying specific changes as described by the text.\n",
        "\n",
        "   ‚û°Ô∏è **Reward: +1 to final grade**\n",
        "\n",
        "---\n",
        "\n",
        "4. **Replacing Text Encoder with Transformer:**  \n",
        "- Replace the text encoder with a Transformer model.\n",
        "- Test the new architecture to ensure it performs text-to-image editing correctly, by satisfying the expect output condition.\n",
        "\n",
        "   ‚û°Ô∏è **Reward: Access to AI Lab**\n",
        "\n",
        "---\n",
        "\n",
        "Make sure to carefully follow the instructions provided in each cell to meet the requirements!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeb5t4EiSld1"
      },
      "source": [
        "## **1. Required: Task Description**\n",
        "\n",
        "Your task is to implement a custom neural network architecture along with its forward pass function.\n",
        "\n",
        "This task is inspired by **text-to-image generation**, where a neural network maps a sequence of tokens representing textual information into a high-dimensional image. The text input is typically **tokenized** into a sequence of integers. This representation can be passed through an **encoder-decoder** architecture to generate images.\n",
        "\n",
        "For this task, you will work with a simplified text-to-image representation in the form of a random tensor with the shape **(1, 10)**:\n",
        "- The 1 indicates that there is a single input sample.\n",
        "- 10 corresponds to the sequence length of the input text tokens.\n",
        "\n",
        "Your implemented model will:\n",
        "- Take this text token tensor as input.\n",
        "- Encode it into a latent representation.\n",
        "- Decode the latent representation to produce an output **image tensor of shape (1, 3, 256, 256)**, where:\n",
        "    - 1 represents the batch size.\n",
        "    - 3 indicates the RGB color channels of the image.\n",
        "    - 256 √ó 256 corresponds to the height and width of the output image.\n",
        "\n",
        "The primary objective is to correctly implement the neural network architecture and its forward pass to achieve the desired functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV5xi065Fk1x"
      },
      "source": [
        "To better view the architecture diagram:  \n",
        "- **Right-click the image** and select **\"Open image in a new tab\"** to enable zoom for a clearer view.  \n",
        "- Alternatively, you can **download the image** using the link below:  \n",
        "  [Download Architecture Diagram](https://drive.google.com/file/d/1osVNVcsNGo-d9DCGVH1hJDw2nw4rMToR/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### Diagram Preview:\n",
        "![Architecture Diagram](https://drive.google.com/uc?export=view&id=1osVNVcsNGo-d9DCGVH1hJDw2nw4rMToR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFHVdn0GF95L"
      },
      "source": [
        "Necessary Imports and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAofci_7R6kB"
      },
      "outputs": [],
      "source": [
        "# Cell 0.1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgOK-xATFdDW",
        "outputId": "b02f6b96-8239-47da-b771-543113ad2e92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Cell 0.2 (GPU is not needed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M35QxrIgfug8"
      },
      "source": [
        "### Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVP_ivSyOfkQ",
        "outputId": "b0e8f95b-ad99-4694-a490-6e07d3eed0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "# Cell 0.3 -> INPUT (DO NOT EDIT THIS CELL!)\n",
        "vocab_size = 10\n",
        "input_tokens = torch.randint(0, vocab_size, (1, 10))\n",
        "print(input_tokens.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99eJ_wATFRv7"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqE_d_vzQZmH"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # ADD YOUR CODE HERE\n",
        "        self.embedding = nn.Embedding(10, 256)\n",
        "        self.lstm = nn.LSTM(256, 128, num_layers=2, batch_first=True)\n",
        "        self.linear = nn.Linear(128, 1024)\n",
        "\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        print(\"encoder started...\")\n",
        "        print(\"Text input:\", input_tokens.shape)\n",
        "        # ADD YOUR CODE HERE\n",
        "        after_embedding = self.embedding(input_tokens)\n",
        "        print(\"After embedding:\", after_embedding.shape)\n",
        "\n",
        "        after_lstm, _ = self.lstm(after_embedding)\n",
        "        print(\"After LSTM:\", after_lstm.shape)\n",
        "\n",
        "        after_linear = self.linear(after_lstm)\n",
        "        print(\"after linear: \", after_linear.shape)\n",
        "\n",
        "        reshaped = after_linear.view(1, 10, 32, 32)\n",
        "        print(\"Reshaped:\", reshaped.shape)\n",
        "\n",
        "\n",
        "        # Reshape to match expected output shape (1, 10, 32, 32)\n",
        "\n",
        "\n",
        "        return reshaped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1ilsWmEHY-8"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "class ImageDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageDecoder, self).__init__()\n",
        "\n",
        "        #top layer\n",
        "        self.T_convTrans2d = nn.ConvTranspose2d(10, 16, kernel_size=12, stride=4, padding=0)\n",
        "\n",
        "\n",
        "        #Middle top\n",
        "        self.MT_conv2d_1 = nn.Conv2d(10, 32, kernel_size=3, stride=1, padding=0)\n",
        "        self.MT_maxPool_1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        #relu\n",
        "        self.MT_batchNorm2d = nn.BatchNorm2d(32)\n",
        "        self.MT_MaxPool_2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        #middle bottom\n",
        "        self.MB_conv2d_1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
        "        #relu\n",
        "        self.MB_batchNorm2d = nn.BatchNorm2d(32)\n",
        "        self.MB_conv2d_2 = nn.Conv2d(32, 32, kernel_size=8, stride=3, padding=0)\n",
        "\n",
        "        #bottom\n",
        "        self.B_conv2d_1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.B_convTrans_1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=0)\n",
        "\n",
        "\n",
        "        #final stretch\n",
        "        self.final_convTrans_1 = nn.ConvTranspose2d(32, 16, kernel_size=24, stride=19, padding=1) #tweak stride\n",
        "        #concat\n",
        "        self.final_convtrans2d_1 = nn.ConvTranspose2d(32, 8, kernel_size=3, stride=2, padding=1)\n",
        "        self.final_conv2d_2 = nn.Conv2d(8, 3, kernel_size=16, stride=1, padding=0) #tweak padding\n",
        "\n",
        "\n",
        "    def forward(self, text_embedding):\n",
        "        print(\"decoder started\")\n",
        "        print(\"Text embedding:\", text_embedding.shape)\n",
        "\n",
        "        #top layer\n",
        "        top_1 = self.T_convTrans2d(text_embedding)\n",
        "        print(\"Top 1:\", top_1.shape)\n",
        "\n",
        "        #middle top layer\n",
        "        middle_conv2d_1 = self.MT_conv2d_1(text_embedding)\n",
        "        print(\"Middle conv2d 1:\", middle_conv2d_1.shape)\n",
        "        middle_maxPool_1 = self.MT_maxPool_1(middle_conv2d_1)\n",
        "        print(\"Middle maxPool 1:\", middle_maxPool_1.shape)\n",
        "        middle_relu_1 = nn.ReLU()(middle_maxPool_1)\n",
        "        print(\"Middle relu 1:\", middle_relu_1.shape)\n",
        "        middle_batchNorm2d = self.MT_batchNorm2d(middle_relu_1)\n",
        "        print(\"Middle batchNorm2d:\", middle_batchNorm2d.shape)\n",
        "        middle_maxPool_2 = self.MT_MaxPool_2(middle_batchNorm2d)\n",
        "        print(\"Middle maxPool 2:\", middle_maxPool_2.shape)\n",
        "\n",
        "\n",
        "        #middle bottom\n",
        "        middle_conv2d_2 = self.MB_conv2d_1(middle_conv2d_1)\n",
        "        print(\"Middle conv2d 2:\", middle_conv2d_2.shape)\n",
        "        middle_relu_2 = nn.ReLU()(middle_conv2d_2)\n",
        "        print(\"Middle relu 2:\", middle_relu_2.shape)\n",
        "        middle_batchNorm2d_2 = self.MB_batchNorm2d(middle_relu_2)\n",
        "        print(\"Middle batchNorm2d_2: \", middle_batchNorm2d_2.shape)\n",
        "        #wait for add with bottom\n",
        "\n",
        "\n",
        "        #bottom\n",
        "        bottom_conv2d_1 = self.B_conv2d_1(middle_conv2d_2)\n",
        "        print(\"Bottom conv2d 1:\", bottom_conv2d_1.shape)\n",
        "        bottom_convTrans_1 = self.B_convTrans_1(bottom_conv2d_1)\n",
        "        print(\"Bottom convTrans 1:\", bottom_convTrans_1.shape)\n",
        "\n",
        "        #add bottom and middle bottom\n",
        "        added_bottom_middle = bottom_convTrans_1 + middle_batchNorm2d_2\n",
        "        print(\"Added bottom and middle:\", added_bottom_middle.shape)\n",
        "\n",
        "        #final middle bottom layer\n",
        "        final_middleBottom = self.MB_conv2d_2(added_bottom_middle)\n",
        "        print(\"Final middle bottom:\", final_middleBottom.shape)\n",
        "\n",
        "        #add with middle top\n",
        "        added_BM_BT = final_middleBottom + middle_maxPool_2\n",
        "        print(\"Added BM and BT:\", added_BM_BT.shape)\n",
        "\n",
        "        #final stretch\n",
        "        final_stretch_1 = self.final_convTrans_1(added_BM_BT)\n",
        "        print(\"Final stretch 1:\", final_stretch_1.shape)\n",
        "\n",
        "        #concat with top layer\n",
        "        concat = torch.cat((top_1, final_stretch_1), dim=1)\n",
        "        print(\"Concat:\", concat.shape)\n",
        "\n",
        "\n",
        "        #final stretch continues\n",
        "        final_stretch_2 = self.final_convtrans2d_1(concat)\n",
        "        print(\"Final stretch 2:\", final_stretch_2.shape)\n",
        "\n",
        "        output = self.final_conv2d_2(final_stretch_2)\n",
        "        print(\"Output:\", output.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENO4kK0X6N5S"
      },
      "source": [
        "#### Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um3Aa5zXIqAt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "bc376b4b-5b20-4082-f8fc-f4059be6301b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder:\n",
            "encoder started...\n",
            "Text input: torch.Size([1, 10])\n",
            "After embedding: torch.Size([1, 10, 256])\n",
            "After LSTM: torch.Size([1, 10, 128])\n",
            "after linear:  torch.Size([1, 10, 1024])\n",
            "Reshaped: torch.Size([1, 10, 32, 32])\n",
            "Decoder:\n",
            "decoder started\n",
            "Text embedding: torch.Size([1, 10, 32, 32])\n",
            "Top 1: torch.Size([1, 16, 136, 136])\n",
            "Middle conv2d 1: torch.Size([1, 32, 30, 30])\n",
            "Middle maxPool 1: torch.Size([1, 32, 15, 15])\n",
            "Middle relu 1: torch.Size([1, 32, 15, 15])\n",
            "Middle batchNorm2d: torch.Size([1, 32, 15, 15])\n",
            "Middle maxPool 2: torch.Size([1, 32, 7, 7])\n",
            "Middle conv2d 2: torch.Size([1, 32, 28, 28])\n",
            "Middle relu 2: torch.Size([1, 32, 28, 28])\n",
            "Middle batchNorm2d_2:  torch.Size([1, 32, 28, 28])\n",
            "Bottom conv2d 1: torch.Size([1, 64, 26, 26])\n",
            "Bottom convTrans 1: torch.Size([1, 32, 28, 28])\n",
            "Added bottom and middle: torch.Size([1, 32, 28, 28])\n",
            "Final middle bottom: torch.Size([1, 32, 7, 7])\n",
            "Added BM and BT: torch.Size([1, 32, 7, 7])\n",
            "Final stretch 1: torch.Size([1, 16, 28, 28])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 136 but got size 28 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d4377da14726>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Decoder:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimage_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-7fa7445578a1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_embedding)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m#concat with top layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_stretch_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concat:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 136 but got size 28 for tensor number 1 in the list."
          ]
        }
      ],
      "source": [
        "#DO NOT MODIFY THIS CELL\n",
        "\n",
        "embedding_dim = 256\n",
        "print(\"Encoder:\")\n",
        "text_encoder = TextEncoder(vocab_size, embedding_dim)\n",
        "text_embedding = text_encoder(input_tokens)\n",
        "print(\"Decoder:\")\n",
        "image_decoder = ImageDecoder()\n",
        "image = image_decoder(text_embedding)\n",
        "\n",
        "try:\n",
        "    assert text_embedding.shape == (1, 10, 32, 32), \"Encoded text shape is incorrect.\"\n",
        "    assert image.shape == (1, 3, 256, 256), \"Decoded image shape is incorrect.\"\n",
        "    print(\"\\nüéâ Congratulations! Your implementation is correct. You passed the minimum requirement! üéâ\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNJYNiAMYSwQ"
      },
      "source": [
        "## **2. Optional: +1 to the Final Grade**\n",
        "- Add another input tensor: a random tensor of size (1, 3, 256, 256).\n",
        "- Implement an Image Encoder with a few layers to encode the tensor. The encoding process should follow the example image provided.\n",
        "- Combine the encoded image embeddings with the text embeddings using cross-attention, following the example image provided.\n",
        "\n",
        "You should only add these new parts and reuse the ImageDecoder previously created. The final output should still be the same as in the previously required task (1,3,256,256)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5hswDE_GmH4"
      },
      "source": [
        "To better view the architecture diagram:  \n",
        "- **Right-click the image** and select **\"Open image in a new tab\"** to enable zoom for a clearer view.  \n",
        "- Alternatively, you can **download the image** using the link below:  \n",
        "  [Download Architecture Diagram](https://drive.google.com/file/d/1nIgqhyPq0eKWEvT7leqoa0ZeBApCCs6u/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### Diagram Preview:\n",
        "![Architecture Diagram](https://drive.google.com/uc?export=view&id=1nIgqhyPq0eKWEvT7leqoa0ZeBApCCs6u)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U55r4Zw74Jgn"
      },
      "source": [
        "#### New Input - create a random tensor of size (1,3,256,256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8mmiPG-4ax-"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abMcDR7t4mXE"
      },
      "source": [
        "#### Image Encoder - create the image encoder, following the example provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7upryOE4ZKE"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, new_image):\n",
        "        print(\"New image:\", new_image.shape)\n",
        "        # ADD YOUR CODE HERE\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_nxf1x24qla"
      },
      "source": [
        "#### Combine with Cross-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjQTQIYp4wn2"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=1024):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, text_embedding, image_embedding):\n",
        "        # ADD YOUR CODE HERE\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDA0I3Bb6Kis"
      },
      "source": [
        "#### Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-xa3jT26S1U"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "image_encoder = ImageEncoder()\n",
        "image_embedding = image_encoder(new_image)\n",
        "\n",
        "embed_dim = 1024\n",
        "cross_attention = CrossAttention(embed_dim=embed_dim)\n",
        "combine = cross_attention(text_embedding, image_embedding)\n",
        "\n",
        "image = image_decoder(combine)\n",
        "\n",
        "try:\n",
        "    assert image_embedding.shape == (1, 10, 32, 32), \"Encoded image shape is incorrect.\"\n",
        "    assert combine.shape == (1, 10, 32, 32), \"Combined cross attention shape is incorrect.\"\n",
        "    assert image.shape == (1, 3, 256, 256), \"Combined cross attention shape is incorrect.\"\n",
        "    print(\"\\nüéâ Congratulations! Your implementation is correct. You increased your final grade by 1! üéâ\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637zP7iiaagl"
      },
      "source": [
        "## **3. Optional: Access to AI Lab**\n",
        "- Replace the text encoder with a Transformer model. This involves:\n",
        "    - Maximizing the sequence length to 16.\n",
        "    - Using BertTokenizer.\n",
        "    - Adding Positional Encoding.\n",
        "    - Defining a Transformer Encoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrIb6M38en7m"
      },
      "source": [
        "To better view the architecture diagram:  \n",
        "- **Right-click the image** and select **\"Open image in a new tab\"** to enable zoom for a clearer view.  \n",
        "- Alternatively, you can **download the image** using the link below:  \n",
        "  [Download Architecture Diagram](https://drive.google.com/file/d/1bwrPryFGAAFF3OoJ3Z7UUzpVJaYvijUg/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### Diagram Preview:\n",
        "![Architecture Diagram](https://drive.google.com/uc?export=view&id=1bwrPryFGAAFF3OoJ3Z7UUzpVJaYvijUg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ONfR0xRdE2"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=16):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG655fyIen7n"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Transformer Encoder\n",
        "class TransformerTextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=16, num_heads=4, num_layers=2):\n",
        "        super(TransformerTextEncoder, self).__init__()\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, input_text):\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oSF35-ven7n"
      },
      "outputs": [],
      "source": [
        "\n",
        "#DO NOT MODIFY THIS CELL\n",
        "new_image = torch.randn((1,3,256,256))\n",
        "embedding_dim = 1024 # 32x32\n",
        "print(\"Encoder:\")\n",
        "text_encoder = TransformerTextEncoder(embedding_dim)\n",
        "input_text = [\"Generate an image based on this text\"]\n",
        "text_embedding = text_encoder(input_text)\n",
        "\n",
        "image_encoder = ImageEncoder()\n",
        "image_embedding = image_encoder(new_image)\n",
        "\n",
        "embed_dim = 1024\n",
        "cross_attention = CrossAttention(embed_dim=embed_dim)\n",
        "combine = cross_attention(text_embedding, image_embedding)\n",
        "\n",
        "print(\"Decoder:\")\n",
        "image_decoder = ImageDecoder()\n",
        "image = image_decoder(combine)\n",
        "\n",
        "try:\n",
        "    assert text_embedding.shape == (1, 16, 32, 32), \"Encoded text shape is incorrect.\"\n",
        "    assert image_embedding.shape == (1, 16, 32, 32), \"Encoded image shape is incorrect.\"\n",
        "    assert combine.shape == (1, 16, 32, 32), \"Combined cross attention shape is incorrect.\"\n",
        "    assert image.shape == (1, 3, 256, 256), \"Decoded image shape is incorrect.\"\n",
        "    print(\"\\nüéâ Congratulations! Your implementation is correct. You passed the requirement for the AI Lab! üéâ\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}